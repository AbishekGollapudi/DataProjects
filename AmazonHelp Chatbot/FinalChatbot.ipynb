{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Implementation with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to train a simple chatbot to handle common questions that Amazon customer agents have to answer such as questions regarding order delays, refunds, etc.\n",
    "\n",
    "The following resources played a large role in helping me building this model:\n",
    "\n",
    "[Seq2Seq Tutorial by Ben Trevett](https://github.com/bentrevett/pytorch-seq2seq)\n",
    "\n",
    "[TorchText and Seq2Seq Tutorial by Adam Wearne](https://medium.com/@adam.wearne/lets-get-sentimental-with-pytorch-dcdd9e1ea4c9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "9dYGaoH4sMbN",
    "outputId": "9b21b29e-af57-4274-b509-b3d8af0e7d34"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#os.chdir('/content/gdrive/My Drive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CM8EiEiKsXMj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_A_gCsAw1CA"
   },
   "source": [
    "### **Using Torchtext to Pre-process Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "GtJv587Yt3FC",
    "outputId": "d3d510a7-d3c1-48d4-8acd-0a80b2cf98f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon fireTVstick</td>\n",
       "      <td>Fire TV Stick https://t.co/2pbG55qJ7h ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 different people have given 3 different answ...</td>\n",
       "      <td>We'd like to take a further look into this wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Way to drop the ball on customer service so pi...</td>\n",
       "      <td>I'm sorry we've let you down! Without providin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want my amazon payments account CLOSED. dm m...</td>\n",
       "      <td>I am unable to affect your account via Twitter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay danke f r die Info</td>\n",
       "      <td>Wir haben zu danken. Sch nen Abend noch.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0                                 amazon fireTVstick   \n",
       "1  3 different people have given 3 different answ...   \n",
       "2  Way to drop the ball on customer service so pi...   \n",
       "3  I want my amazon payments account CLOSED. dm m...   \n",
       "4                            Okay danke f r die Info   \n",
       "\n",
       "                                             answers  \n",
       "0           Fire TV Stick https://t.co/2pbG55qJ7h ET  \n",
       "1  We'd like to take a further look into this wit...  \n",
       "2  I'm sorry we've let you down! Without providin...  \n",
       "3  I am unable to affect your account via Twitter...  \n",
       "4           Wir haben zu danken. Sch nen Abend noch.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('chatbot_data2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tUhkWAftHUC"
   },
   "source": [
    "**Model Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpTau8lltDuU"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 30_000\n",
    "MIN_COUNT = 3\n",
    "MAX_SEQUENCE_LENGTH = 15\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXCE0xrZtPfS"
   },
   "source": [
    "**Create Field object and load csv into tabular dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3jkp9MPtL-J"
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=my_tokenizer, lower=True, include_lengths=True, init_token='<sos>', eos_token='<eos>')\n",
    "# Setup Field to tokenize using spacy, and include <sos> and <eos> tokens\n",
    "\n",
    "fields = [('input_sequence', TEXT), ('output_sequence', TEXT)] #use FIELD processing on each column (both are text based)\n",
    "\n",
    "filepath = 'chatbot_data2.csv'\n",
    "\n",
    "table_data = data.TabularDataset(path=filepath, format='csv', fields=fields)  #turn data into torchtext's TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmMokMBgvw0F"
   },
   "source": [
    "**Build Vocabulary**\n",
    "\n",
    "By default, *UNK* tokens are tokens that appear in initial dataset vocabulary but not the Glove embedding.\n",
    "\n",
    "These *UNK* tokens are set to tensors of zero, but training is faster if they are initialized to some random values instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9yfWw3ruXiW"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(table_data,\n",
    "                max_size=MAX_VOCAB_SIZE, #30_000\n",
    "                min_freq=MIN_COUNT, #3 \n",
    "                vectors='glove.6B.300d',\n",
    "                unk_init=torch.Tensor.normal_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Zjg32I4Gya3c",
    "outputId": "4f16404e-c1fa-4bbe-dbe6-4a1cedba9b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in vocabulary: 30004\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in vocabulary: {len(TEXT.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stHGUfK2wx7e"
   },
   "source": [
    "**Train-Test Split and Bucket Iterator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Ix-R-FiwRzi"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = table_data.split()\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_d2WEayVzaTJ",
    "outputId": "a091e8dd-f406-4b2d-ad7b-c8ecba69f6c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thanks', 'for', 'this', 'is', 'there', 'anyway', 'to', 'get', 'a', 'refund']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].__dict__.keys()\n",
    "train_data[0].input_sequence[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvqotLGMxfXa"
   },
   "source": [
    "Bucket Iterator essentially batches together similar samples such that there is only a minimum amount of padding. Iterator basically processes the data so that it can easily be input into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtufJrBOxaFQ"
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x:len(x.input_sequence),\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zM9TUOUu6L2q"
   },
   "source": [
    "### **Encoder**\n",
    "\n",
    "<img src=\"https://docs.chainer.org/en/stable/_images/seq2seq.png\"  width=\"500\" height=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XX1SwWDnPC9b"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  \n",
    "    def __init__(self, hidden_dims, embedding_size,\n",
    "                 embedding, num_layers=2, dropout=0.0):\n",
    "      \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Basic network params\n",
    "        self.hidden_dims = hidden_dims  #dimensionality of hidden state\n",
    "        self.embedding = embedding #embedding layer using GloVe\n",
    "        self.embedding_size = embedding_size #dimensionality of embedding layer (Vocab size)\n",
    "        self.num_layers = num_layers #num stacked RNN layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Bidirectional LSTM\n",
    "        self.gru = nn.GRU(embedding_size,hidden_dims,\n",
    "                          num_layers=num_layers,\n",
    "                          dropout=dropout,\n",
    "                          bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_sequence, input_lengths):\n",
    "        word_embeddings = self.embedding(input_sequence) #turn tokens into word embeddings\n",
    "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(word_embeddings, input_lengths) #pad sequence to max length\n",
    "        outputs, hidden = self.gru(packed_embeddings) #Run padded sequence through LSTM RNN\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs) #Unpack sequence to only get inputs w/o <PAD> tokens\n",
    "        outputs = outputs[:, :, :self.hidden_dims] + outputs[:, : ,self.hidden_dims:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYb8cPOJA4BK"
   },
   "source": [
    "### **Attention Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKx2BJORJb42"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dimensions):\n",
    "        super(Attention, self).__init__() \n",
    "        self.hidden_dimensions = hidden_dimensions\n",
    "        \n",
    "    def dot_score(self, hidden_state, encoder_states):\n",
    "        return torch.sum(hidden_state * encoder_states, dim=2) #get dot product of decoder hidden state vs. encoder states\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "       \n",
    "        attn_scores = self.dot_score(hidden, encoder_outputs)\n",
    "# Transpose max_length and batch_size dimensions\n",
    "        attn_scores = attn_scores.t()\n",
    "# Apply mask so network does not attend <pad> tokens        \n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, -1e10)   \n",
    "# Return softmax over attention scores      \n",
    "        return F.softmax(attn_scores, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcNUztJ9C9Ok"
   },
   "source": [
    "### **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGFXJMKePRiG"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding, embedding_size,\n",
    "                 hidden_dims, output_size, n_layers=1, dropout=0.1):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Basic network params\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = embedding\n",
    "                \n",
    "        self.gru = nn.GRU(embedding_size, hidden_dims, n_layers, \n",
    "                          dropout=dropout)\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_dims * 2, hidden_dims)\n",
    "        self.out = nn.Linear(hidden_dims, output_size)\n",
    "        self.attn = Attention(hidden_dims)\n",
    "        \n",
    "    def forward(self, current_token, hidden_state, encoder_outputs, mask):\n",
    "      \n",
    "        # convert current_token to word_embedding\n",
    "        embedded = self.embedding(current_token) #turn token into word embedding\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        rnn_output, hidden_state = self.gru(embedded, hidden_state) #pass word embedding through GRU\n",
    "        \n",
    "        # Get attention distribution after passing through softmax layer\n",
    "        attention_weights = self.attn(rnn_output, encoder_outputs, mask) \n",
    "\n",
    "        # Matrix multiply attention weights vs. encoder outputs to get context words \n",
    "        context = attention_weights.bmm(encoder_outputs.transpose(0, 1)) #ma\n",
    "        \n",
    "        # Concatenate context vector and LSTM output\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        \n",
    "        # Pass concat_output to final output layer\n",
    "        output = self.out(concat_output)\n",
    "        \n",
    "        # Return output and final hidden state\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4S8wihGPXfE"
   },
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_dims, vocab_size, \n",
    "                 device, pad_token, eos_token, sos_token, teacher_forcing_ratio=0.5):\n",
    "        super(seq2seq, self).__init__()\n",
    "        \n",
    "        # Embedding layer shared by encoder and decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Initialize Encoder network\n",
    "        self.encoder = Encoder(hidden_dims, \n",
    "                               embedding_size, \n",
    "                               self.embedding,\n",
    "                              num_layers=2,\n",
    "                              dropout=0.5)\n",
    "        \n",
    "        # Initalize Decoder network        \n",
    "        self.decoder = Decoder(self.embedding,\n",
    "                               embedding_size,\n",
    "                              hidden_dims,\n",
    "                              vocab_size,\n",
    "                              n_layers=2,\n",
    "                              dropout=0.5)\n",
    "        \n",
    "        \n",
    "        # Indices of special tokens and hardware device \n",
    "        self.pad_token = pad_token\n",
    "        self.eos_token = eos_token\n",
    "        self.sos_token = sos_token\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, input_sequence):\n",
    "        return (input_sequence != self.pad_token).permute(1, 0)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_sequence, output_sequence, teacher_forcing_ratio=0.5):\n",
    "      \n",
    "        # Unpack input_sequence tuple\n",
    "        input_tokens = input_sequence[0]\n",
    "        input_lengths = input_sequence[1]\n",
    "      \n",
    "        # Unpack output_tokens, or create an empty tensor for text generation\n",
    "        if output_sequence is None:\n",
    "            inference = True\n",
    "            output_tokens = torch.zeros((100, input_tokens.shape[1])).long().fill_(self.sos_token).to(self.device)\n",
    "        else:\n",
    "            inference = False\n",
    "            output_tokens = output_sequence[0]\n",
    "\n",
    "        vocab_size = self.decoder.output_size\n",
    "        batch_size = len(input_lengths)\n",
    "        max_seq_len = len(output_tokens)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_seq_len, batch_size, vocab_size).to(self.device)\n",
    "        \n",
    "        \n",
    "        # Pass through the first half of the network\n",
    "        encoder_outputs, hidden = self.encoder(input_tokens, input_lengths)\n",
    "        \n",
    "        # Ensure dim of hidden_state can be fed into Decoder\n",
    "        hidden =  hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = output_tokens[0,:]\n",
    "        \n",
    "        # Create mask\n",
    "        mask = self.create_mask(input_tokens)\n",
    "        \n",
    "        \n",
    "        # Step through the length of the output sequence one token at a time\n",
    "        # Teacher forcing is used to assist training\n",
    "        for t in range(1, max_seq_len):\n",
    "            output = output.unsqueeze(0)\n",
    "            \n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (output_tokens[t] if teacher_force else top1)\n",
    "            \n",
    "            # If we're in inference mode, keep generating until we produce an\n",
    "            # <eos> token\n",
    "            if inference and output.item() == self.eos_token:\n",
    "                return outputs[:t]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4wqBvp1IOkT"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-24deeb9681e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<sos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "pad_token = TEXT.vocab.stoi['<pad>']\n",
    "eos_token = TEXT.vocab.stoi['<eos>']\n",
    "sos_token = TEXT.vocab.stoi['<sos>']\n",
    "\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "model = seq2seq(embedding_dim,\n",
    "                 hidden_dim, \n",
    "                 vocab_size, \n",
    "                 device, pad_token, eos_token, sos_token).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_token = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_token] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_token] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True]\n",
    "                       , lr=1.0e-3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VoO1-hCYIVbc"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, optimizer, clip=1.0):\n",
    "   # Put the model in evaluation mode!\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for index, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "\n",
    "            input_sequence = batch.input_sequence\n",
    "            output_sequence = batch.output_sequence\n",
    "            if index == 0:\n",
    "                print(input_sequence)\n",
    "\n",
    "            target_tokens = output_sequence[0]\n",
    "\n",
    "            # Run the batch through our model\n",
    "            output = model(input_sequence, output_sequence)\n",
    "\n",
    "            # Throw it through our loss function\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            target_tokens = target_tokens[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, target_tokens)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0rwbKHEQ08n"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion, optimizer, clip=1.0):\n",
    "   # Put the model in training mode!\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for index, batch in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        \n",
    "        input_sequence = batch.input_sequence\n",
    "        output_sequence = batch.output_sequence\n",
    "        \n",
    "        target_tokens = output_sequence[0]\n",
    "        \n",
    "        # zero out the gradient for the current batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run the batch through our model\n",
    "        output = model(input_sequence, output_sequence)\n",
    "        \n",
    "        # Throw it through our loss function\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        target_tokens = target_tokens[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, target_tokens)\n",
    "        \n",
    "        # Perform back-prop and calculate the gradient of our loss function\n",
    "        loss.backward()\n",
    "          \n",
    "        # Clip the gradient if necessary.          \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJDU1o9tKl9r"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWUSnmYAKnl6"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5xsek9wQpuZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING\n"
     ]
    }
   ],
   "source": [
    "print('WORKING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "CLIP = 50.0\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "_-hixYFWKpoX",
    "outputId": "0952db84-8046-4bab-e844-fc61d5fe4666"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "CLIP = 50.0\n",
    "best_valid_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion, optimizer, CLIP)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-second-model.pt') \n",
    "        with open('epoch_curr.txt', 'w') as file:\n",
    "            file.write(str(epoch))\n",
    "        \n",
    "        \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        torch.save(model.state_dict(), 'best-train-model.pt') \n",
    "        with open('epoch_curr_train.txt', 'w') as file:\n",
    "            file.write(str(epoch))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JtW3UavtKrn2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('epoch_curr.txt', 'r')\n",
    "file.read()\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('test.txt', 'w') as file:\n",
    "    file.write(str(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=90\n",
    "file = open('epoch_curr.txt', 'w')\n",
    "file.write(str(num))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('epoch_curr.txt', 'r')\n",
    "file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def translate_sentence(model, sentence, nlp):\n",
    "    model.eval()\n",
    "    \n",
    "    tokenized = nlp(sentence) \n",
    "    \n",
    "    tokenized = ['<sos>'] + [t.lower_ for t in tokenized] + ['<eos>']\n",
    "    numericalized = [TEXT.vocab.stoi[t] for t in tokenized] \n",
    "    \n",
    "    sentence_length = torch.LongTensor([len(numericalized)]).to(model.device) \n",
    "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(model.device) \n",
    "    \n",
    "    translation_tensor_logits = model((tensor, sentence_length), None, 0) \n",
    "    \n",
    "    #print(len(sentence))\n",
    "    #print(\"SHAPE:\", translation_tensor_logits.squeeze(1).shape)\n",
    "\n",
    "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1) \n",
    "    #print(torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
    "    translation = [TEXT.vocab.itos[t] for t in translation_tensor]\n",
    " \n",
    "    # Start at the first index.  We don't need to return the <sos> token...\n",
    "    translation = translation[1:]\n",
    "    return translation, translation_tensor_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi, how can I help you?\n",
      " \n",
      "Enter: My package was stolen\n",
      " \n",
      "Chatbot: Oh no! I'm sorry for the trouble! ! Please reach us here : https://t.co/haplpmlfhn so we can look into this with you.\n",
      " \n",
      "Enter: my order has been delayed for a week!\n",
      " \n",
      "Chatbot: I'm sorry for the delay! What does the tracking show for the order? You can check here : https://t.co/y5jpi9grhe\n",
      " \n",
      "Enter: it still says on the way!\n",
      " \n",
      "Chatbot: Thanks for confirming. Please reach out to us here so we can look into this with you : https://t.co/haplpmlfhn\n",
      " \n",
      "Enter: ok thank you!\n",
      " \n",
      "Chatbot: You're welcome! Let us know if you need any other questions or concerns.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "first = 0\n",
    "\n",
    "def cap(match):\n",
    "    return(match.group().capitalize())\n",
    "\n",
    "def my_capitalize(cap, s):\n",
    "    p = re.compile(r'((?<=[\\.\\?!]\\s)(\\w+)|(^\\w+))')\n",
    "    result = p.sub(cap, s)\n",
    "    return re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', result)\n",
    "\n",
    "\n",
    "while(1):\n",
    "    if first==0:\n",
    "        print('Chatbot: Hi, how can I help you?')\n",
    "        print(' ')\n",
    "    first+=1\n",
    "    input_sent = input('Enter: ')\n",
    "    \n",
    "    if input_sent == 'q' or input_sent == 'quit': break\n",
    "\n",
    "    elif input_sent == 'hello':\n",
    "        print('Chatbot: Hello!')\n",
    "    else:\n",
    "        response, logits = translate_sentence(model, input_sent, nlp)\n",
    "        print(' ')\n",
    "        temp = \" \".join(response)\n",
    "        print(\"Chatbot: \" + my_capitalize(cap, temp))\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-9a2196c3bc1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "s = \"i'm sorry for the delay ! what does the tracking show for the order?\"\n",
    "punctuation = ['!', '?', '.']\n",
    "\n",
    "sentences = sent_tokenizer.tokenize(s)\n",
    "sentences = [sent.capitalize() for sent in sentences]\n",
    "pprint(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry for the delay ! What does the tracking show for the order ? You can check here : https://t.co/y5jpi9grhe\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(r'(?<=[\\.\\?!]\\s)(\\w+)')\n",
    "\n",
    "s = \"i'm sorry for the delay ! what does the tracking show for the order ? you can check here : https://t.co/y5jpi9grhe\"\n",
    "               \n",
    "def cap(match):\n",
    "    return(match.group().capitalize())\n",
    "\n",
    "def my_capitalize(cap, s):\n",
    "    p = re.compile(r'((?<=[\\.\\?!]\\s)(\\w+)|(^\\w+))')\n",
    "    return p.sub(cap, s)\n",
    "\n",
    "my_capitalize(cap,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm sorry for the delay ! I'm sorry for the delay ! what does the tracking show for the order ? you can check here : https://t.co/y5jpi9grhe does the tracking show for the order ? I'm sorry for the delay ! what does the tracking show for the order ? you can check here : https://t.co/y5jpi9grhe can check here : https://t.co/y5jpi9grhe\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 291\n",
    "\n",
    "print(df.sample(10, random_state=n)['questions'].iloc[1])\n",
    "print(df.sample(10, random_state=n)['answers'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('chatbot_data2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()['questions'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'im so sorry for this!'\n",
    "\n",
    "def check_sorry(s):\n",
    "    if 'sorry' in str(s):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "check_sorry(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sorrys'] = df['answers'].apply(check_sorry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sorrys'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinalChatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
